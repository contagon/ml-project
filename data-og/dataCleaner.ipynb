{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for downloading then unzipping the data files\n",
    "~~~\n",
    "pip install kaggle\n",
    "export KAGGLE_USERNAME=eastonpotokar\n",
    "export KAGGLE_KEY=1bfd140356aae8ffd62b234617295b77\n",
    "kaggle datasets download -d shuyangli94/food-com-recipes-and-user-interactions\n",
    "unzip food-com-recipes-and-user-interactions.zip\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean out the Recipes Dataset, extracting the items that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save map from ingredients id to ingredients\n",
    "maps = pickle.load( open('ingr_map.pkl', 'rb') )\n",
    "ing_decoder = {i['id']: i['replaced'] for index, i in maps.iterrows()}\n",
    "pickle.dump(ing_decoder, open(\"../data-cleaned/ingredient_decoder.pkl\", \"wb\"))\n",
    "\n",
    "#we put recipes into similar format as our dataset\n",
    "pp_recipes = pd.read_csv('PP_recipes.csv')\n",
    "raw_recipes = pd.read_csv('RAW_recipes.csv')\n",
    "recipes = pd.merge(pp_recipes, raw_recipes, on='id')\n",
    "\n",
    "#clean and get tags all ready\n",
    "tags = set()\n",
    "for i in raw_recipes.tags:\n",
    "    tags.update( json.loads(i.replace(\"'\", '\"')) )\n",
    "    \n",
    "tags = sorted(list(tags))\n",
    "tag_encoder = {tag: i for i, tag in enumerate(tags)}\n",
    "tag_decoder = {i: tag for i, tag in enumerate(tags)}\n",
    "pickle.dump(tag_decoder, open(\"../data-cleaned/tag_decoder.pkl\", \"wb\"))\n",
    "\n",
    "#also make a recipe decoder\n",
    "recipe_decoder = {i['i']: i['name'] for _, i in recipes.iterrows()}\n",
    "pickle.dump(recipe_decoder, open(\"../data-cleaned/recipe_decoder.pkl\", \"wb\"))\n",
    "\n",
    "#get the number of everything we're doing\n",
    "num_rec = len(list(recipes['i']))\n",
    "num_ing = max(ing_decoder.keys())+1\n",
    "num_tags = max(tag_decoder.keys())+1\n",
    "num_others = 1 #calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays to use for calculating tf-idf\n",
    "row_sum = np.zeros(num_rec)\n",
    "col_sum = np.zeros(num_ing+num_tags)\n",
    "\n",
    "#iterate through, saving the tags and ingredients\n",
    "row_recipe = []\n",
    "column_item = []\n",
    "for _, recipe in recipes.iterrows():\n",
    "    #get all ingredients\n",
    "    for i in json.loads(recipe['ingredient_ids']):\n",
    "        row_recipe.append(recipe['i'])\n",
    "        column_item.append(i)\n",
    "        row_sum[recipe['i']] += 1\n",
    "        col_sum[i] += 1\n",
    "    #get all tags\n",
    "    for j in json.loads(recipe['tags'].replace(\"'\", '\"')):\n",
    "        row_recipe.append(recipe['i'])\n",
    "        column_item.append(tag_encoder[j] + num_ing)\n",
    "        row_sum[recipe['i']] += 1\n",
    "        col_sum[tag_encoder[j] + num_ing] += 1\n",
    "\n",
    "#one of each ingredient and tag added\n",
    "counts = [1]*len(row_recipe)\n",
    "\n",
    "#### MINUTES seemed very noisy - information also included in tags. we remove them b/c of it\n",
    "#add in minutes\n",
    "# row_recipe += list(recipes['i'])\n",
    "# column_item += [num_ing+num_tags]*len(list(recipes.index))\n",
    "# counts += list(recipes[\"minutes\"])\n",
    "\n",
    "### CALORIES also very noisy - we use \"calorie level\" instead\n",
    "#add in calories\n",
    "row_recipe += list(recipes['i'])\n",
    "column_item += [num_ing+num_tags]*len(list(recipes.index))\n",
    "counts += list(recipes['calorie_level'])\n",
    "# counts += [json.loads(i)[0] for i in list(recipes['nutrition'])]\n",
    "\n",
    "#save unscaled matrix\n",
    "X = sparse.csr_matrix((counts, [row_recipe, column_item]), shape=(num_rec, num_ing + num_tags + num_others), dtype=np.float)\n",
    "sparse.save_npz('../data-cleaned/recipes.npz', X)\n",
    "\n",
    "#do the scaling\n",
    "for i, (row, col) in enumerate(zip(row_recipe, column_item)):\n",
    "    #don't rescale the calories/minutes yet\n",
    "    if col == num_ing+num_tags+1 or col == num_ing+num_tags:\n",
    "        continue\n",
    "    counts[i] = (counts[i] / row_sum[row])*np.log(num_rec/col_sum[col])\n",
    "    \n",
    "#save scaled matrix\n",
    "Xhat = sparse.csr_matrix((counts, [row_recipe, column_item]), shape=(num_rec, num_ing + num_tags + num_others), dtype=np.float)\n",
    "sparse.save_npz('../data-cleaned/recipes_tfidf.npz', Xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the users dataset, extracting what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we'll load all the data\n",
    "user_train = pd.read_csv('interactions_train.csv')\n",
    "user_valid = pd.read_csv('interactions_validation.csv')\n",
    "user_test  = pd.read_csv('interactions_test.csv')\n",
    "\n",
    "#TRAIN SET save the user number, recipe and rating\n",
    "num_user = max(user_train['u'].max(), user_valid['u'].max(), user_test['u'].max())+1\n",
    "row_user = list(user_train['u'])\n",
    "column_rec = list(user_train['i'])\n",
    "rating = np.array(list(user_train['rating']))\n",
    "rating[rating==0] = 1\n",
    "rating = list(rating)\n",
    "\n",
    "#make and save\n",
    "X = sparse.csr_matrix((rating, [row_user, column_rec]), shape=(num_user, num_rec), dtype=np.float)\n",
    "sparse.save_npz('../data-cleaned/user_train.npz', X)\n",
    "\n",
    "\n",
    "#sum rows and columns\n",
    "row_sum = np.squeeze( np.array(X.sum(axis=1)) )\n",
    "col_sum = np.squeeze( np.array(X.sum(axis=0)) )\n",
    "\n",
    "#do the scaling\n",
    "for i, (row, col) in enumerate(zip(row_user, column_rec)):\n",
    "    if rating[i] != 0:\n",
    "        rating[i] = (rating[i] / row_sum[row])*np.log(num_user/col_sum[col])\n",
    "\n",
    "#save scaled matrix\n",
    "Xhat = sparse.csr_matrix((rating, [row_user, column_rec]), shape=(num_user, num_rec), dtype=np.float)\n",
    "sparse.save_npz('../data-cleaned/user_train_tfidf.npz', Xhat)\n",
    "\n",
    "\n",
    "#TRAIN SET save the user number, recipe and rating\n",
    "temp = pd.concat((user_valid, user_test))\n",
    "row_user = list(temp['u']) + list(temp['u'].unique())\n",
    "column_rec = list(temp['i']) + [num_rec]*len(temp['u'].unique())\n",
    "rating_temp = np.array(list(temp['rating']))\n",
    "rating_temp[rating_temp==0] = 1\n",
    "rating = list(rating_temp) + list(temp['u'].unique()) \n",
    "\n",
    "#make and save\n",
    "X = sparse.csr_matrix((rating, [row_user, column_rec]), shape=(num_user, num_rec+1), dtype=np.float)\n",
    "\n",
    "#remove nonzero rows\n",
    "X = X[X.getnnz(1)>0]\n",
    "sparse.save_npz('../data-cleaned/user_test.npz', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train User Set seems good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44367,  87844, 101723, 134551], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = sparse.load_npz(\"../data-cleaned/user_train.npz\")\n",
    "U[22095].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, 5.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[22095, 44367], U[22095, 87844]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flank steak with lime chipotle sauce', 'greek stuffed meatloaf')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_decoder[44367], recipe_decoder[87844]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Test set also seems good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173538 178265]\n",
      "4.0 2.0\n"
     ]
    }
   ],
   "source": [
    "U = sparse.load_npz(\"../data-cleaned/user_test.npz\")\n",
    "print(U[0].nonzero()[1])\n",
    "print(U[0, 173538], U[0, 178265])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to decode tags from array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4-hours-or-less',\n",
       " '5-ingredients-or-less',\n",
       " 'asian',\n",
       " 'course',\n",
       " 'cuisine',\n",
       " 'dietary',\n",
       " 'easy',\n",
       " 'indian',\n",
       " 'main-ingredient',\n",
       " 'occasion',\n",
       " 'pasta-rice-and-grains',\n",
       " 'preparation',\n",
       " 'rice',\n",
       " 'side-dishes',\n",
       " 'time-to-make',\n",
       " 'vegan',\n",
       " 'vegetarian']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sparse.load_npz(\"../data-cleaned/recipes.npz\")\n",
    "tags = X[23].nonzero()[1]\n",
    "tags = tags[tags>num_ing][:-2] - num_ing\n",
    "[tag_decoder[i] for i in tags[:-1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
